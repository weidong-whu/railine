\section{Methodology}

The flow of our methodology is presented in Figure 1.
We take aerial images of the railway area as input,
for which SfM and point clouds are used in advanced with existing software.
We first extract the image line and convert it to a space line with the locally optimal plane of the point cloud.
Then,
we cluster the single 3D line to RT candidates with the frame work derived from DBSCAN,  
during which the texture information of multiple images extracted from ResNet is used as one of the inlier distance.
Having obtained the RT cluster, 
we then trace and reconstruct the vector-based RT in the Kalman framwork,
which fully exploits the RT structure and the multi-view geometries to resolve the uncertainty caused by initial image line segment extraction and the point cloud error.
The railway-track pair is the start seed of our Kalman method.
We first convert image lines to 3D lines with the local optimal plane of the point cloud.
Then,
we cluster the single 3D line to RT pairs with DBSCAN frame work,  
during which the deep feature of multiple images is used as the inlier distance.
Since the 




\subsection{Railway track with Kalman filter}
As shown in, 
we use two points and directions to represent the state of the local \textit{RLP}.
Denote the point and direction as
$\mathbf p=\left(x,y,z\right)$ and $\mathbf d=\left(dx,dy,dz\right)$,
respectively.
The vector to be estimated for the \textit{RLP} is
\begin{equation}
\mathbf x = \begin{bmatrix}
    \mathbf p_1 & \mathbf p_2 & \mathbf d_1  & \mathbf d_2 
\end{bmatrix}^ \top \in R^{12}.
\end{equation}
The prediction of $\mathbf x$ is controlled by a scalar $t$:
\begin{equation}
        \mathbf{x}^{pre}= 
        \mathrm F \mathbf x^{{\mbox -}}, \in R^{12}, \quad  
        \mathrm F=\operatorname{diag}\left(t \! \cdot \! \mathrm I_{6\times6} , \mathrm I_{6\times6}\right) \in R^{12\times 12},
        \label {eq_statetransition}
\end{equation}
where the superscript $^{\mbox -}$ marks the previous state.
For each prediction $\mathbf{x}^{pre}$,
There is an actual observation $\mathbf{x}^{obs}$ arising from line reconstruction in multiple images~(\cref*{sec_linereconstruction}).
The \textit{RLP} has fixed geometry patterns,
i.e.,
$\mathbf d_1$ and $\mathbf d_2$ should be as close as possible,
and the distance change between $\mathbf p_1$ and $\mathbf p_2$ is as small as possible.
We achieve these two constraints by extending the observation vector:
\begin{equation}
\mathbf{z}^{obs} = \begin{bmatrix}
    \mathbf{x}^{obs} & \mathbf p_1^{\mbox -} - \mathbf p_{2}^{\mbox -} & \mathbf 0_{1\times3}
\end{bmatrix}^ \top \in R^{18}.
\label {eq_observationvector}
\end{equation}
Correspondingly,
the observation matrix that translate $\mathbf{x}^{pre}$ to the observation form is
\begin{equation}
        \mathbf{z}^{pre}= 
        \mathrm H \mathbf x^{pre}, \quad  
        \mathrm H=
        \begin{bmatrix}
            \multicolumn{4}{c}{\mathrm{I}_{12 \times 12}} \\
            \mathrm{I}_{3 \times 3} & -\mathrm{I}_{3 \times 3} & \mathbf{0}_{3 \times 3} & \mathbf{0}_{3 \times 3} \\
            \mathbf{0}_{3 \times 3} & \mathbf{0}_{3 \times 3} & \mathrm{I}_{3 \times 3} & -\mathrm{I}_{3 \times 3}
        \end{bmatrix} \in R^{18 \times 12}.
        \label {eq_observationmatrix }
\end{equation}
Then,
as shown in fig.1,
we use the general discrete Kalman filter to update the state:
\begin{figure}[!ht]
\centering
\resizebox{0.75\textwidth}{!}{%
\begin{circuitikz}
\tikzstyle{every node}=[font=\normalsize]
\draw  (0,23.5) rectangle  
node {\normalsize
\begin{minipage}{5cm}
(b)~$\mathbf{z}^{obs}$ (\cref{eq_observationvector}) calculation via
3D line reconstruction.  
\end{minipage}
} (6.25,21.5);
\draw  (0,20.5) rectangle  
node {\normalsize  
\begin{minipage}{5cm}
(a)~$\mathbf{x}^{pre}$ prediction  with \cref{eq_statetransition}
and error covariance prediction via 
\begin{equation}
\mathrm P^{pred}= \mathrm F \mathrm P^{\mbox -} \mathrm F^{\top}+\mathrm Q
\end{equation}
\end{minipage}
}  (6.25,18.5);
\draw  (7.5,23.5) rectangle  
node {
\normalsize 
\begin{minipage}{5cm}
(c)~Kalman gain update :
\begin{equation}
\mathrm K= \mathrm P \mathrm H^{\top}
\left(\mathrm H \mathrm P \mathrm H^{\top}+\mathrm R\right)^{-1}
\end{equation}
(d)~estimation update:
\begin{equation}
\mathbf{\hat{x}}= \mathbf x^{pre} + \mathrm K \left(\mathbf{z}^{pre}- \mathbf{z}^{obs}\right)
\end{equation}
(e)~error covariance update:
\begin{equation}
\mathrm{P} = \left( \mathrm{I} - \mathrm{K} \mathrm{H} \right) \mathrm{P}^{pre}
\end{equation}
 \end{minipage}
} (13.5,18.5);
\draw [->, >=Stealth] (6.25,22.5) -- (7.5,22.5);
\draw [->, >=Stealth] (7.5,19.5) -- (6.25,19.5);
\draw [->, >=Stealth] (3.3,20.5) -- (3.3,21.5);
\draw [->, >=Stealth] (-1.25,19.5) -- (0,19.5);
\draw [<-, >=Stealth] (-1.25,22.5) -- (0,22.5);
\draw [draw=none] (-4.5,16.7) rectangle  
node {\normalsize
\begin{minipage}{6cm}
Initial estimate of $\mathbf {\hat{x}}$ and $\mathrm {P}$
\end{minipage}
} (-1,22.5);
\draw [draw=none] (-6,22.4) rectangle  
node {\normalsize
\begin{minipage}{4cm}
Check for termination:
\begin{itemize}
    \vspace{-0.5em}
    \item non-convergence of reconstruction
    \vspace{-0.5em}
    \item visual inconsistency
    \vspace{-0.5em}
    \item positional overlap 
\end{itemize}
\end{minipage}
} (-2,22.4);
\end{circuitikz}
}%
\label{fig:my_label}
\end{figure}


\subsection{Accurate railway line reconstruction}
\label{sec_linereconstruction}

\subsection{The seed generation for railway track}
We construct the rough 3D line based on the dense point cloud.
Given the end point $\mathbf p$ of a 2D line segment,
we randomly sample three space points around $\mathbf p$ to construct the plane $\pi$,
and we cast a ray $\mathbf r$ passing through $\mathbf p$ from the camera center.
Then,
the 3D point candidate $\mathrm P \in R ^ {3\times1}$ for $\mathbf p$ can be obtained by $\mathbf r$-to-$\pi$ intersection,
and the candidate 3D line $\mathrm L$ can be represented by the two 3D point:
\begin{equation}
    L =\left\{\textit{inter} \left(\pi,\mathbf r_1\right),\textit{inter} \left(\pi,\mathbf r_2\right)  \right\},
\end{equation}
where $\mathbf r_1$ and $\mathbf r_2$ are the rays of the two endpoints,
and \textit{inter} calculates the ray-to-plane intersection.
After random sampling of $n$ times,
we obtain a set of candidate 3D lines $\left\{ L\right\}_{i=1}^n$ and use the LMEDS algorithm,
which does not require an inlier threshold, to confirm the best 3D line for a 2D line:
\begin{equation}
    L^* = \arg\min_{L_i} \text{median} \left\{ d_{ij}\right\}_{j=1}^n ,
\end{equation}
where $d_{ij}$ is the projection distance between $L_i$ and $L_j$.  

We group two 3D lines as a RT pair based on their angle $\theta_{i,j}$,
overlap $o_{i,j}$,
and projection distance $d_{i,j}$:
\begin{equation}
   \left\{ RT= \left(L_i, L_j\right) \mid \theta_{i,j} < t_\theta, o_{i,j} > t_o, d_{i,j} \in I  \right\},
    \label{eq_geometrycons}
\end{equation}
$\theta_{i,j}$ and $o_{i,j}$are easy to choose,
e.g.,
$5^\circ$ and 60\%,
because the RT pair is parallel and highly overlapped;
while the interval $I$ needs the rough width $\omega$ between the two RT,
which can be acquired from construction standards or point clouds.
We recommend setting $I=\left[2/3\omega,4/3\omega\right]$ that uses one-third of $\omega$ as the margin of error.
Because a 3D line may satisfy \cref{eq_geometrycons} with many others,
the greedy algorithm is used to assign the candidate pair,
which uses the sum of the overlap rate as the maximum score.

We sort the RT based on their scores of the geometry alignment and select the top 10\% RT and use contextual information to further validate the RT pair.
In detail,
if the RT's central line is within $1^\circ$ and $t$ projection distance with another RT,
its score is increased by $\mathcal{N}\left(\mu, \left(t/3\right)^2\right)$.
we use the global average pooling layer in ResNet50 to describe the feature of the RT pair.
Because it has been trained on massive amounts of data and can capture texture information for classification in the absence of labels.
Also, 
we re-transform the image blocks to reduce the ambiguity caused by scale and rotation.
After extraction of RT features, 
we use DBSCAN to group them with the cosine distance,
and retain the group with the highest number as the seeds of RT.












